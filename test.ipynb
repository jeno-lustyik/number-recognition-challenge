{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5),(0.5))])\n",
    "\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = T.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = T.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.input_layer = nn.Conv2d(1,28,kernel_size=7)\n",
    "        self.conv1 = nn.Conv2d(28, 56, 7)\n",
    "        self.conv2 = nn.Conv2d(56,112,7)\n",
    "        self.drop = nn.Dropout2d(0.1)\n",
    "        self.layer1 = nn.Linear(112*10*10,60)\n",
    "        self.layer2 = nn.Linear(60,30)\n",
    "        self.layer3 = nn.Linear(30,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.drop(self.input_layer(x)))\n",
    "        x = F.relu(self.drop(self.conv1(x)))\n",
    "        x = F.relu(self.drop(self.conv2(x)))\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        out_layer = self.layer3(x)\n",
    "        return out_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (input_layer): Conv2d(1, 28, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (conv1): Conv2d(28, 56, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (conv2): Conv2d(56, 112, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (drop): Dropout2d(p=0.1, inplace=False)\n",
       "  (layer1): Linear(in_features=11200, out_features=60, bias=True)\n",
       "  (layer2): Linear(in_features=60, out_features=30, bias=True)\n",
       "  (layer3): Linear(in_features=30, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 784]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rnr31\\Documents\\GitHub\\number-recognition-challenge\\test.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnr31/Documents/GitHub/number-recognition-challenge/test.ipynb#ch0000006?line=19'>20</a>\u001b[0m T\u001b[39m.\u001b[39msqueeze(images\u001b[39m.\u001b[39mresize_(images\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], \u001b[39m784\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnr31/Documents/GitHub/number-recognition-challenge/test.ipynb#ch0000006?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rnr31/Documents/GitHub/number-recognition-challenge/test.ipynb#ch0000006?line=23'>24</a>\u001b[0m logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(images)   \u001b[39m# 1) Forward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnr31/Documents/GitHub/number-recognition-challenge/test.ipynb#ch0000006?line=24'>25</a>\u001b[0m pred \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlog_softmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnr31/Documents/GitHub/number-recognition-challenge/test.ipynb#ch0000006?line=25'>26</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred, labels) \u001b[39m# 2) Compute loss\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\rnr31\\Documents\\GitHub\\number-recognition-challenge\\test.ipynb Cell 3'\u001b[0m in \u001b[0;36mClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnr31/Documents/GitHub/number-recognition-challenge/test.ipynb#ch0000002?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rnr31/Documents/GitHub/number-recognition-challenge/test.ipynb#ch0000002?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layer(x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnr31/Documents/GitHub/number-recognition-challenge/test.ipynb#ch0000002?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rnr31/Documents/GitHub/number-recognition-challenge/test.ipynb#ch0000002?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pipcv22\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pipcv22\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/conv.py?line=446'>447</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pipcv22\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/rnr31/anaconda3/envs/pipcv22/lib/site-packages/torch/nn/modules/conv.py?line=443'>444</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [64, 784]"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "print_every = 40\n",
    "final_trl = []\n",
    "final_tel =[]\n",
    "final_accuracy = []\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    epoch_accurracy =[]\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        # images.resize_(images.size()[0], 784)\n",
    "        T.squeeze(images.resize_(images.size()[0], 784))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model.forward(images)   # 1) Forward pass\n",
    "        pred = F.log_softmax(logits, dim=1)\n",
    "        loss = criterion(pred, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    with T.no_grad():\n",
    "        for i, (images, labels) in enumerate(iter(testloader)):\n",
    "            images.resize_(images.size()[0], 784)\n",
    "\n",
    "            logits = model.forward(images)\n",
    "            test_pred = F.log_softmax(logits, dim=1)\n",
    "\n",
    "            test_loss = criterion(test_pred, labels)\n",
    "\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "            top_p, top_class = test_pred.topk(k=1, dim=1)\n",
    "            equals = top_class == labels[i].view(*top_class.shape)\n",
    "            misclassified = [index for index,value in enumerate(equals) if value.item() is False] \n",
    "            accuracy = T.mean(equals.type(T.FloatTensor))\n",
    "            epoch_accurracy.append(accuracy.item()*100)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    mean_trl = mean(train_losses)\n",
    "    mean_tel = mean(test_losses)\n",
    "    mean_acc = mean(epoch_accurracy)\n",
    "    final_trl.append(mean_trl)\n",
    "    final_tel.append(mean_tel)\n",
    "    final_accuracy.append(mean_acc)\n",
    "    \n",
    "\n",
    "    #accuracies.append(acc)\n",
    "    print(f'Epoch: {e + 1} | loss: {loss.item()} | test loss: {test_loss.item()} | accuracy:{accuracy.item()*100} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(final_trl, label='train Loss')\n",
    "plt.plot(final_tel, label='test Loss')\n",
    "#plt.plot(accuracies, label='accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6a88286319d0d529453aaad86e23161652ae20ee8331519045b47d8d7f809e4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pipcv22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
